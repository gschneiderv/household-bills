{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414df8d8-951d-4563-9469-61792f43ba8f",
   "metadata": {},
   "source": [
    "For importing delta you need to make sure that you have selected the right kernel\n",
    "\n",
    "If the below cell doesn't work, you might need to install the kernel first.\n",
    "\n",
    "For doing so, follow these steps:\n",
    "\n",
    "1. conda install ipykernel\n",
    "2. python -m ipykernel install --user --name=bills --display-name \"Python (bills)\"\n",
    "\n",
    "Now you should be able to select the Python (bills) kernell which will be pointing to the conda environment\n",
    "containing the libraries for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21d67f-3b15-4ffa-b01f-04e578d942a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import itemgetter\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "\n",
    "from pyspark.sql.functions import lit, input_file_name, split, col\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547938a-b429-49db-bf7e-7adc93e26ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37860ae2-f8ed-44fe-bd9d-be8180cf79d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc429d6-e175-41c3-86cc-bc82afa83a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Print the Python version\n",
    "print(sys.version)\n",
    "\n",
    "# Print only the major, minor, and micro versions\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c098c49-0502-4876-ad0a-59728f7d62d3",
   "metadata": {},
   "source": [
    "The above uses method configure_spark_with_delta_pip from the delta library which installs the necessaries\n",
    "maven dependencies for the underlying pyspark process. The implementation of the function can be found here:\n",
    "\n",
    "https://github.com/delta-io/delta/blob/da162a097a25524fc97334f47a180257cb487789/python/delta/pip_utils.py#L23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deccc0f-8286-465a-bf56-ae8d3aacb5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession \\\n",
    "     .builder \\\n",
    "     .master(\"local[2]\") \\\n",
    "     .config('spark.cores.max', '3') \\\n",
    "     .config('spark.executor.memory', '2g') \\\n",
    "     .config('spark.executor.cores', '2') \\\n",
    "     .config('spark.sql.catalogImplementation', 'hive') \\\n",
    "     .config('spark.driver.memory', '1g') \\\n",
    "     .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "     .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "     .enableHiveSupport()\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51f75c-d3e1-4259-b146-17887dbe1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "spark \\\n",
    "\t.read \\\n",
    "    .text(\"/Users/andreslaurito/repos/household-bills/household-bills/tickets\", wholetext=True)\\\n",
    "    .withColumn(\"__LoadID\", lit(str(uuid.uuid4()))) \\\n",
    "\t.withColumn(\"__DCR\", lit(str(now)).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"_file_name\", input_file_name()) \\\n",
    "    .createOrReplaceTempView(\"raw_tickets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4569fc-6513-4c29-a5e2-1faee4e57e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Dia matches a lot of things bc of its meaning :)\n",
    "supermarkets = ['mercadona', 'aldi', 'grupo dia',\n",
    "                'primark', 'prenatal', 'zeeman',\n",
    "                'condis', 'carrefour', 'consum',\n",
    "                'miscota', 'farmacia', 'veritas',\n",
    "                'lidl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34e760-ffe1-48d9-b93e-74a54fe93f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tickets = spark.sql(\"select * from raw_tickets\")\n",
    "\n",
    "all_tickets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ac47c-0935-4d24-98f0-31ef9f300b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM raw_tickets where not (\"\n",
    "for supermarket in supermarkets:\n",
    "    query += f\"value ilike '%{supermarket}%' or \"\n",
    "\n",
    "query = query[:-4]\n",
    "query += ')'\n",
    "\n",
    "tickets_not_belonging_to_supermarkets = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372f96e5-6bc0-443a-a794-fbec15d434d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_not_belonging_to_supermarkets.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad1795-e396-4565-b200-714913bcc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_supermarkets = {\n",
    "    supermarket: spark.sql(f\"SELECT * FROM raw_tickets where value ilike '%{supermarket}%'\")\n",
    "    for supermarket in supermarkets\n",
    "}\n",
    "\n",
    "def total_spent(df):\n",
    "    imports_per_ticket = []\n",
    "    for row in df.select('splitted').collect():\n",
    "        for word in row.splitted:\n",
    "            if 'Import' in word:\n",
    "                wordmatch = re.findall(\"\\d+\\,\\d+\", word)\n",
    "                if wordmatch:\n",
    "                    imports_per_ticket.append(float(wordmatch[0].replace(',', '.')))\n",
    "        #         re.\n",
    "        # ticket_import = [[0] for word in row.splitted if 'Import' in word]\n",
    "        # imports_per_ticket.extend([float(importnum.replace(',', '.')) for importnum in ticket_import])\n",
    "    return sum(imports_per_ticket)\n",
    "\n",
    "\n",
    "info_per_supermarket = {\n",
    "    supermarket: {\n",
    "        'df': df,\n",
    "        'count': df.count(),\n",
    "        'df_splitted': df.withColumn('splitted', split(col('value'), '\\r\\n')),\n",
    "        'total_spent': total_spent(df.withColumn('splitted', split(col('value'), '\\r\\n')))\n",
    "    } for supermarket, df in df_per_supermarkets.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaecc3-c149-4b72-8e4f-d19aec924c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info_per_supermarket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba3e5-c76b-4ce5-84ad-45a13ec69b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_per_supermarket['mercadona']['df'].show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9e4d50-b954-486a-95dc-a705f91078c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_most_tickets = sorted([(supermarket, value['count']) for supermarket, value in info_per_supermarket.items()],\n",
    "                                   key=itemgetter(1), reverse=True)\n",
    "\n",
    "supermarkets_most_spent = sorted([(supermarket, value['total_spent']) for supermarket, value in info_per_supermarket.items()],\n",
    "                                  key=itemgetter(1), reverse=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff6bb0-8d95-4273-bd27-0d1ffaf78f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_most_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0f0f3-e71c-45a0-b814-f556e296957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarkets_most_spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a93c28-a203-470a-b4fc-8a4d284478a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mercadona.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676a5f9-5c24-441f-88d3-a3fc008d4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, StringType, DateType\n",
    "\n",
    "# Sample input data\n",
    "# Define the UDF function to extract ticket data\n",
    "def extract_ticket_data(text):\n",
    "    rows = []\n",
    "    ticket_data = text.split(\"\\r\\n\")\n",
    "    recipe_date = \"\"\n",
    "    for i in range(len(ticket_data)):\n",
    "        product_quantity = \"\"\n",
    "        product_name = \"\"\n",
    "        unit_price = \"\"\n",
    "        total_price = \"\"\n",
    "        \n",
    "        # Match pattern for product and name\n",
    "        date_pattern = re.compile(r\"\\d+\\/\\d+\\/\\d+\")\n",
    "        regex_matched = date_pattern.search(ticket_data[i])\n",
    "        if regex_matched:\n",
    "            # Should match only once per recipe\n",
    "            recipe_date = regex_matched[0]\n",
    "            \n",
    "        if re.match(r\"\\d \\w+\", ticket_data[i]):\n",
    "            regex_match = re.match(r\"(\\d+)( )(\\w+(\\s\\w+)*)\", ticket_data[i])\n",
    "            if regex_match:\n",
    "                product_quantity = regex_match[1]\n",
    "                product_name = regex_match[3]\n",
    "\n",
    "                # Find unit and total price in subsequent lines\n",
    "                unit_price_idx = i + 1\n",
    "                total_price_idx = i + 2\n",
    "\n",
    "                unit_price = ticket_data[unit_price_idx] if unit_price_idx < len(ticket_data) and re.match(r\"\\d+,\\d+\", ticket_data[unit_price_idx]) else \"\"\n",
    "                total_price = ticket_data[total_price_idx] if total_price_idx < len(ticket_data) and re.match(r\"\\d+,\\d+\", ticket_data[total_price_idx]) else \"\"\n",
    "                    \n",
    "                # Append extracted data\n",
    "                rows.append((product_quantity, product_name, unit_price, total_price, recipe_date))\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Define UDF with return type as ArrayType of StructType (list of rows)\n",
    "schema = StructType([\n",
    "    StructField(\"product_quantity\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"unit_price\", StringType(), True),\n",
    "    StructField(\"total_price\", StringType(), True),\n",
    "    StructField(\"recipe_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Initialize an empty DataFrame with the defined schema\n",
    "mercadona_products_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Iterate through each row in the original DataFrame and apply the UDF\n",
    "for row in df_mercadona.collect():\n",
    "    extracted_data = extract_ticket_data(row['value'])\n",
    "    print(extracted_data)\n",
    "    if extracted_data:\n",
    "        # Convert the extracted data to a DataFrame\n",
    "        temp_df = spark.createDataFrame(extracted_data, schema)\n",
    "        # Append the new rows to the new DataFrame\n",
    "        mercadona_products_df = mercadona_products_df.union(temp_df)\n",
    "\n",
    "\n",
    "mercadona_products_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"mercadona_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23307e-b762-4ca4-aa46-81009f3ff1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE mercadona_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31432f0-1a41-4919-8f6b-de54af101ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM mercadona_products\").show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9bd11-c90a-473b-94f0-e12e5e4567e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "bananas_df = spark.sql(\"SELECT * FROM mercadona_products where product_name ilike '%banana%'\")\n",
    "\n",
    "bananas_with_dates = bananas_df.withColumn(\"recipe_date_asdate\", F.to_date(\"recipe_date\", \"dd/MM/yyyy\")).drop(\"recipe_date\")\n",
    "\n",
    "bananas_with_dates.sort(\"recipe_date_asdate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7ad16-dee5-42dc-8d88-3b5e4b3fae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "bananas_with_dates_cleaned = bananas_with_dates.withColumn(\n",
    "    \"total_price_cleaned\", \n",
    "    F.regexp_replace(\"total_price\", r\"[^\\d,\\.]\", \"\")  # Remove all non-numeric characters except ',' and '.'\n",
    ")\n",
    "\n",
    "# Convert the cleaned 'total_price' column to a numeric type\n",
    "bananas_with_dates_cleaned = bananas_with_dates_cleaned.withColumn(\n",
    "    \"total_price_int\", \n",
    "    F.when(\n",
    "        F.trim(F.col(\"total_price_cleaned\")) != \"\", \n",
    "        F.regexp_replace(F.col(\"total_price_cleaned\"), \",\", \".\").cast(\"double\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "# Assuming 'bananas_with_dates' is your PySpark DataFrame\n",
    "# Convert 'recipe_date_asdate' to timestamp (numeric format)\n",
    "bananas_with_dates = bananas_with_dates_cleaned.withColumn(\"recipe_date_timestamp\", F.unix_timestamp(\"recipe_date_asdate\", \"yyyy-MM-dd\"))\n",
    "\n",
    "bananas_with_dates.show()\n",
    "\n",
    "# Convert to Pandas DataFrame for easy plotting\n",
    "pandas_df = bananas_with_dates.select(\"recipe_date_asdate\", \"total_price_int\").toPandas()\n",
    "\n",
    "# Clean the 'total_price' column to ensure it's a numeric type\n",
    "pandas_df[\"total_price\"] = pd.to_numeric(pandas_df[\"total_price_int\"], errors='coerce')  # Convert invalid values to NaN\n",
    "\n",
    "print(pandas_df)\n",
    "\n",
    "# Plotting the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pandas_df[\"recipe_date_asdate\"], pandas_df[\"total_price_int\"], color='blue', label='Total Price')\n",
    "plt.title(\"Scatter Plot of Total Price vs Recipe Date\")\n",
    "plt.xlabel(\"Recipe Date (Unix Timestamp)\")\n",
    "plt.ylabel(\"Total Price\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670229a5-88d8-4c89-92e1-86e4da7bdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM mercadona_products where product_name ilike '%salmo%'\").show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36e2e5-9c91-47b0-ae01-f3c4701a8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM mercadona_products where product_name ilike '%ANARCARDS%'\").show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c5b10-1db9-441f-8adf-93f1866c22bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bills)",
   "language": "python",
   "name": "bills"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
